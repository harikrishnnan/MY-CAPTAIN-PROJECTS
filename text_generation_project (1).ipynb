{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import dependencies\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "#Project gutenberg/burg is where the data can be found\n",
    "file=open('frankenstein-2.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization-is essentially breaking down a stram of text into words,phrases or symbols or other such meaningfull elements called tokens.\n",
    "#standardization\n",
    "#tokenization is the process of breaking a stram of text up into words phrases symbols or other meaningful elements\n",
    "def tokenize_words(input):\n",
    "    #lowercase everything to standarize it\n",
    "    input=input.lower()\n",
    "    #initializing the tokenizer\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    #tokenizing the text into tokens\n",
    "    \n",
    "    tokens=tokenizer.tokenize(input)\n",
    "    #filtering the stopwords using lambda\n",
    "    filtered=filter(lambda token:token not in stopwords.words('english'),tokens)\n",
    "    return \" \".join(filtered)\n",
    "processed_inputs=tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chars to numbers\n",
    "chars=sorted(list(set(processed_inputs)))\n",
    "char_to_num=dict((c,i) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters 269995\n",
      "total vocab: 43\n"
     ]
    }
   ],
   "source": [
    "#check if words to chars to num(?!) has worked?\n",
    "input_len=len(processed_inputs)\n",
    "vocab_len=len(chars)\n",
    "print(\"total number of characters\",input_len)\n",
    "print(\"total vocab:\",vocab_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq length\n",
    "seq_length=100\n",
    "x_data=[]\n",
    "y_data=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patterns 269895\n"
     ]
    }
   ],
   "source": [
    "#loop through the sequence\n",
    "for i in range(0,input_len - seq_length,1):\n",
    "    in_seq=processed_inputs[i:i+seq_length]\n",
    "    out_seq=processed_inputs[i+seq_length]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "n_patterns=len(x_data)\n",
    "print(\"total patterns\",n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269895\n"
     ]
    }
   ],
   "source": [
    "X=np.reshape(x_data,(n_patterns,seq_length,1))\n",
    "X=X/float(vocab_len)\n",
    "print(X.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269895\n"
     ]
    }
   ],
   "source": [
    "#one hot encoding\n",
    "y=np_utils.to_categorical(y_data)\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "model=Sequential()\n",
    "model.add(LSTM(256,input_shape=(X.shape[1],X.shape[2]),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256,return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1],activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights\n",
    "filepath=\"mode_weighs_saved.hdf5\"\n",
    "checkpoint=ModelCheckpoint(filepath,monitor='loss',verbose=1,save_best_only=True,mode='min')\n",
    "desired_callbacks=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1055/1055 [==============================] - ETA: 0s - loss: 2.9200\n",
      "Epoch 00001: loss improved from inf to 2.92002, saving model to mode_weighs_saved.hdf5\n",
      "1055/1055 [==============================] - 2308s 2s/step - loss: 2.9200\n",
      "Epoch 2/4\n",
      "1055/1055 [==============================] - ETA: 0s - loss: 2.6376\n",
      "Epoch 00002: loss improved from 2.92002 to 2.63759, saving model to mode_weighs_saved.hdf5\n",
      "1055/1055 [==============================] - 2307s 2s/step - loss: 2.6376\n",
      "Epoch 3/4\n",
      "1055/1055 [==============================] - ETA: 0s - loss: 2.4872\n",
      "Epoch 00003: loss improved from 2.63759 to 2.48719, saving model to mode_weighs_saved.hdf5\n",
      "1055/1055 [==============================] - 2345s 2s/step - loss: 2.4872\n",
      "Epoch 4/4\n",
      "1055/1055 [==============================] - ETA: 0s - loss: 2.3635\n",
      "Epoch 00004: loss improved from 2.48719 to 2.36350, saving model to mode_weighs_saved.hdf5\n",
      "1055/1055 [==============================] - 2338s 2s/step - loss: 2.3635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7117bb37b8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model and let it train\n",
    "model.fit(X,y,epochs=4,batch_size=256,callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recompile the model with same weights\n",
    "filename='mode_weighs_saved.hdf5'\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "##output of the model back into characters\n",
    "num_to_char=dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random seed: \n",
      "' y increase miseries tenfold obstacle wishes ah victor assured cousin playmate sincere love made mise '\n"
     ]
    }
   ],
   "source": [
    "#random seed to help generate\n",
    "start=np.random.randint(0,len(x_data)-1)\n",
    "pattern=x_data[start]\n",
    "print('random seed: ')\n",
    "print('\\'',''.join([num_to_char[value] for value in pattern]),'\\'')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare se"
     ]
    }
   ],
   "source": [
    "#generate the text\n",
    "for i in range(1000):\n",
    "    x=np.reshape(pattern,(1,len(pattern),1))\n",
    "    x=x/float(vocab_len)\n",
    "    prediction=model.predict(x,verbose=0)\n",
    "    index=np.argmax(prediction)\n",
    "    result=num_to_char[index]\n",
    "    seq_in=[num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern=pattern[1:len(pattern)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
